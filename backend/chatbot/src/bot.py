import os
import time
from threading import Thread
from openai import RateLimitError

from .text_processor import chunk_pdfs
from .chroma_db import save_to_chroma_db, CHROMA_PATH
from langchain_chroma import Chroma

from langchain_core.prompts import ChatPromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.messages import HumanMessage, AIMessage


# =========================
# ConfiguraciÃ³n de OpenRouter
# =========================
os.environ["OPENAI_API_KEY"] = "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"  
os.environ["OPENAI_API_BASE"] = "https://openrouter.ai/api/v1"

# =========================
# Carpeta de PDFs
# =========================
BASE_DIR = os.path.dirname(os.path.dirname(__file__))  # â†’ backend/chatbot
PDFS_DIR = os.path.abspath(os.path.join(BASE_DIR, "documents"))
os.makedirs(PDFS_DIR, exist_ok=True)


class ChatBot:
    def __init__(self):
        # =========================
        # Inicializar embeddings
        # =========================
        try:
            print("âš™ï¸ Inicializando embeddings...")
            self.modelo_embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={"local_files_only": True}
            )
            print("âœ… Embeddings cargados correctamente")
        except Exception as e:
            print("ğŸ’¥ Error cargando embeddings:", e)
            self.modelo_embeddings = None

        # =========================
        # Historial
        # =========================
        self.historial = InMemoryChatMessageHistory()

        # =========================
        # Modelo LLM (OpenRouter)
        # =========================
        try:
            print("âš™ï¸ Inicializando modelo LLM en OpenRouter...")
            self.modelo = ChatOpenAI(
                model="meta-llama/llama-4-scout:free",
                temperature=0.1,
                openai_api_key=os.environ["OPENAI_API_KEY"],
                openai_api_base=os.environ["OPENAI_API_BASE"],
            )
            print("âœ… Modelo LLM listo")
        except Exception as e:
            print("ğŸ’¥ Error inicializando modelo LLM:", e)
            self.modelo = None

        # =========================
        # BD Chroma persistente
        # =========================
        try:
            print("ğŸ“¦ Cargando base de datos Chroma persistente...")
            self.db = Chroma(
                persist_directory=CHROMA_PATH,
                embedding_function=self.modelo_embeddings
            )
        except Exception as e:
            print("ğŸ’¥ Error cargando BD Chroma:", e)
            self.db = None

        # PDFs procesados
        self.PROCESADOS = set()
        self._procesar_documentos_iniciales()

        # =========================
        # Hilo de monitoreo de PDFs
        # =========================
        self.thread = Thread(target=self._revisar_pdfs, daemon=True)
        self.thread.start()

    # =========================
    # Procesar PDFs iniciales
    # =========================
    def _procesar_documentos_iniciales(self):
        documentos_iniciales = []
        try:
            for filename in os.listdir(PDFS_DIR):
                if filename.endswith(".pdf"):
                    pdf_path = os.path.join(PDFS_DIR, filename)
                    print(f"ğŸ“„ Procesando {filename}...")
                    chunks = chunk_pdfs(pdf_path)
                    documentos_iniciales.extend(chunks)
                    self.PROCESADOS.add(filename)

            if documentos_iniciales:
                self.db = save_to_chroma_db(documentos_iniciales, self.modelo_embeddings)
                print(f"âœ… Se procesaron {len(documentos_iniciales)} chunks iniciales")
        except Exception as e:
            print("ğŸ’¥ Error procesando PDFs iniciales:", e)

    # =========================
    # Monitoreo continuo de PDFs
    # =========================
    def _revisar_pdfs(self):
        while True:
            try:
                archivos_actuales = {f for f in os.listdir(PDFS_DIR) if f.endswith(".pdf")}

                # Nuevos
                nuevos = archivos_actuales - self.PROCESADOS
                for filename in nuevos:
                    pdf_path = os.path.join(PDFS_DIR, filename)
                    print(f"ğŸ“„ Nuevo PDF detectado: {filename}")
                    chunks = chunk_pdfs(pdf_path)
                    self.db = save_to_chroma_db(chunks, self.modelo_embeddings)
                    self.PROCESADOS.add(filename)

                # Eliminados
                eliminados = self.PROCESADOS - archivos_actuales
                for filename in eliminados:
                    print(f"ğŸ—‘ PDF eliminado: {filename}, borrando de Chroma...")
                    self.db.delete(where={"source": filename})
                    self.PROCESADOS.remove(filename)

            except Exception as e:
                print("ğŸ’¥ Error revisando PDFs:", e)

            time.sleep(30)

    # =========================
    # MÃ©todo principal del chat
    # =========================
    def ask(self, pregunta: str) -> str:
        print("ğŸ¤” Iniciando ask() con la pregunta:", pregunta)
        try:
            documentos_relacionados = self.db.similarity_search_with_score(pregunta, k=5)
            contexto = "\n\n---\n\n".join([doc.page_content for doc, _ in documentos_relacionados])
            print(f"ğŸ“š Documentos recuperados: {len(documentos_relacionados)}")
        except Exception as e:
            print("ğŸ’¥ Error en similarity_search:", e)
            contexto = ""

        historial_texto = "\n".join(
            f"Usuario: {m.content}" if isinstance(m, HumanMessage) else f"Asistente: {m.content}"
            for m in self.historial.messages
        )

        PLANTILLA_PROMPT = """
Eres un asistente llamado PoliChat experto en responder preguntas basadas en documentos proporcionados.

ğŸ”¹ Instrucciones:
- Da respuestas **claras y concisas** (mÃ¡x. 1-2 frases).
- Si corresponde, usa **listas o viÃ±etas** para estructurar la informaciÃ³n.
- No inventes datos que no estÃ©n en el contexto.
- MantÃ©n un tono natural y conversacional.

ğŸ“‚ Contexto disponible:
{context}

ğŸ’¬ Historial de conversaciÃ³n previa:
{chat_history}

â“ Pregunta actual:
{question}

âœï¸ Respuesta:
"""
        try:
            prompt_template = ChatPromptTemplate.from_template(PLANTILLA_PROMPT)
            prompt = prompt_template.format(
                context=contexto,
                chat_history=historial_texto,
                question=pregunta
            )
            print("ğŸ“ Prompt generado (primeros 300 chars):", prompt[:300])
        except Exception as e:
            print("ğŸ’¥ Error generando prompt:", e)
            return "âš  Error generando prompt"

        # Llamada con reintento
        while True:
            try:
                print("âš¡ Llamando al modelo en OpenRouter...")
                respuesta = self.modelo.invoke([HumanMessage(content=prompt)])
                print("âœ… Respuesta recibida del modelo:", respuesta)
                break
            except RateLimitError:
                print("â³ Rate limit, reintentando en 60s...")
                time.sleep(60)
            except Exception as e:
                print("ğŸ’¥ Error llamando al modelo:", e)
                return "âš  Error llamando al modelo"

        # Guardar en historial
        try:
            self.historial.add_message(HumanMessage(content=pregunta))
            self.historial.add_message(AIMessage(content=respuesta.content))
        except Exception as e:
            print("âš  Error guardando en historial:", e)

        return respuesta.content.strip()
